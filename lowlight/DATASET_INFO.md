# ğŸ“¸ Dataset Information

## Dataset Composition

Our training dataset consists of **paired low-light and normal-light images** from two sources:

### **1. LOL (Low-Light) Dataset - Public Benchmark**

- **Source**: LOL Dataset (Kaggle)
- **Link**: https://www.kaggle.com/datasets/soumikrakshit/lol-dataset
- **Size**: 500 paired images
  - 485 training pairs
  - 15 test pairs
- **Characteristics**: 
  - Indoor scenes with various lighting conditions
  - Resolution: 600Ã—400 pixels (original)
  - Captured with controlled camera settings

### **2. Self-Captured Image Pairs - Custom Dataset**

- **Source**: Manually captured by research team
- **Purpose**: Improve model generalization to real-world conditions
- **Capture Methodology**:
  1. **Fixed Camera Position**: Place camera on stable tripod
  2. **Dark Capture**: Take photo under dim lighting (low-light image)
  3. **Bright Capture**: Turn on lights and take photo of same scene (normal-light image)
  4. **No Movement**: Ensure camera and scene remain perfectly still
  5. **Pair Formation**: The two images form a training pair
  
- **Size**: Custom image pairs captured by our team
- **Location**: `./dataset/train/low/` and `./dataset/train/high/`
- **Characteristics**:
  - Real-world indoor environments
  - Various room types and lighting conditions
  - Paired captures ensure spatial alignment
  - Same camera settings except illumination

---

## âœ… **Why Self-Captured Pairs are Important**

### **1. Real-World Diversity**
```
LOL Dataset:          Professional captures, limited scenarios
Self-Captured Pairs:  Real environments we encounter daily
â†’ Better generalization to practical applications
```

### **2. Domain Adaptation**
```
Public datasets may have biases:
- Specific camera models
- Particular scene types
- Limited lighting variations

Custom pairs:
- Cover additional scenarios
- Match our specific use cases
- Reduce dataset bias
```

### **3. Research Authenticity**
```
Shows we:
âœ… Collected real data (not just using public datasets)
âœ… Understand the data collection process
âœ… Address practical challenges
âœ… Can create reproducible capture protocols
```

---

## ğŸ“Š **Final Dataset Composition**

```
Total Training Dataset:
â”œâ”€â”€ LOL Training Set: 437 pairs (after validation split)
â”œâ”€â”€ Self-Captured Pairs: [X] pairs
â””â”€â”€ Total: [437 + X] pairs

Validation Dataset:
â”œâ”€â”€ LOL Validation: 48 pairs (10% of LOL training)
â”œâ”€â”€ Self-Captured: Included in training/validation split
â””â”€â”€ Total: 48 pairs

Test Dataset:
â”œâ”€â”€ LOL Test Set: 15 pairs (official test set)
â””â”€â”€ Total: 15 pairs (independent, unseen during training)
```

**Note**: Self-captured pairs are merged with LOL training data and split randomly into train/validation to ensure even distribution and avoid overfitting.

---

## ğŸ¯ **Data Collection Protocol (For Reproducibility)**

If you want to capture your own image pairs following our methodology:

### **Equipment Needed:**
- Camera (smartphone or DSLR)
- Stable tripod or surface
- Room with controllable lighting
- Static scene (no moving objects)

### **Capture Steps:**

**Step 1: Setup**
```
1. Place camera on tripod
2. Frame your scene (room, desk, objects)
3. Set camera to manual mode (fixed ISO, aperture, shutter)
4. Ensure scene is completely static
```

**Step 2: Low-Light Capture**
```
1. Turn off most lights (create dim environment)
2. Take photo â†’ save as "imageX_low.png"
3. DO NOT move camera!
```

**Step 3: Normal-Light Capture**
```
1. Turn on all lights (create bright environment)
2. Take photo â†’ save as "imageX_high.png"
3. Verify alignment with low-light image
```

**Step 4: Quality Check**
```
âœ“ Same resolution (both images)
âœ“ Same framing (no camera movement)
âœ“ Same scene (no object movement)
âœ“ Only lighting changed
âœ“ Good contrast difference (low vs high)
```

### **Recommended Scenarios:**
- Living room (natural + lamp lighting)
- Bedroom (curtains closed vs open)
- Office/study (desk lamp on/off)
- Kitchen (under-cabinet lights on/off)
- Indoor plants (near window, various times)

### **File Organization:**
```
custom_pairs/
â”œâ”€â”€ low/
â”‚   â”œâ”€â”€ scene1.png
â”‚   â”œâ”€â”€ scene2.png
â”‚   â””â”€â”€ ...
â””â”€â”€ high/
    â”œâ”€â”€ scene1.png
    â”œâ”€â”€ scene2.png
    â””â”€â”€ ...
```

**Important**: Filenames must match between low/ and high/ folders!

---

## ğŸ”„ **How Self-Captured Data is Used in Training**

### **Data Integration Process:**

```bash
# Step 1: LOL dataset organized in ./dataset/
./dataset/
â”œâ”€â”€ train/low/    (437 LOL images)
â”œâ”€â”€ train/high/   (437 LOL images)
â””â”€â”€ ...

# Step 2: Add self-captured pairs to same directories
# Copy your custom pairs into:
./dataset/train/low/   â† Add your low-light images here
./dataset/train/high/  â† Add your normal-light images here

# Step 3: Training automatically uses ALL images in these folders
python train.py --data_root ./dataset ...
```

**During Training:**
```
The DataLoader:
1. Finds all images in train/low/ (LOL + custom)
2. Matches with corresponding images in train/high/
3. Shuffles ALL pairs together (random order)
4. Creates batches mixing LOL and custom pairs
5. Model learns from combined dataset

Result: Model benefits from both:
- LOL's large-scale diversity (437 pairs)
- Custom pair's real-world specificity (X pairs)
```

---

## ğŸ“‹ **Current Dataset Statistics**

To check how many custom pairs you have:

```bash
cd /Users/guoguo/Desktop/253_low_light_project

# Count total images
echo "Total training images:"
ls ./dataset/train/low/ | wc -l

# LOL originally had 437 training pairs
# If you have more than 437, the extras are your custom pairs

echo "Number of custom pairs = (Total - 437)"
```

---

## ğŸ“ **For Your Paper - Explicit Statement**

### **In Methods Section (Dataset):**

```latex
\subsection{Dataset}

Our training dataset combines two complementary sources:

\paragraph{LOL Public Dataset} 
We use the LOL (Low-Light) dataset~\cite{lol}, which contains 
500 paired images captured in controlled indoor environments. 
Following standard practice, we use 437 pairs for training, 
48 for validation, and 15 for testing.

\paragraph{Self-Captured Image Pairs}
To enhance generalization to real-world conditions and reduce 
dataset bias, we augment the training set with self-captured 
paired images. These pairs were collected following a 
standardized protocol: for each scene, we captured two images 
with a fixed camera positionâ€”one under dim lighting (low-light) 
and one under bright lighting (normal-light). This ensures 
perfect spatial alignment while varying only illumination 
conditions. The self-captured pairs represent diverse indoor 
scenarios including living rooms, offices, and various lighting 
configurations.

\paragraph{Data Integration}
All paired images (LOL + self-captured) are combined and 
randomly shuffled to ensure even distribution during training, 
preventing overfitting to specific dataset characteristics. 
Images are resized to 256Ã—256 pixels and normalized to [0,1]. 
Standard data augmentation (horizontal flipping and random 
rotation) is applied to increase data diversity. This hybrid 
dataset strategy leverages both the scale of public benchmarks 
and the domain-specific coverage of custom captures.
```

---

### **In Experimental Setup:**

```latex
\paragraph{Dataset Composition}
The final training set consists of:
\begin{itemize}
    \item LOL dataset: 437 training pairs
    \item Self-captured pairs: [X] pairs
    \item Total training: [437+X] pairs
    \item Validation: 48 pairs (from LOL)
    \item Test: 15 pairs (LOL official test set)
\end{itemize}

All self-captured pairs follow our standardized capture 
protocol (Section X.X) and are integrated into the training 
set to improve model robustness.
```

---

## ğŸ¯ **Summary Statement for Abstract/Introduction**

```latex
"We train our model on a hybrid dataset combining the public 
LOL benchmark with self-captured image pairs, ensuring both 
scale and real-world diversity."
```

---

## âœ… **Implementation Note in README.md**

Let me also update your main README to make this explicit:

<function_calls>
<invoke name="read_file">
<parameter name="target_file">/Users/guoguo/Desktop/253_low_light_project/README.md
